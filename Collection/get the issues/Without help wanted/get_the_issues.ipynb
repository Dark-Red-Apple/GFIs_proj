{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Alma.Z, Dark_red_apple\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "df_contri_rep = pd.read_csv('contri_rep_half.csv',usecols=['url'])\n",
    "# pages = [1,2,3,4,5]\n",
    "username = 'dark-red-apple'\n",
    "\n",
    "# from https://github.com/user/settings/tokens\n",
    "token = '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'\n",
    "for ind in df_contri_rep.index:\n",
    "\n",
    "    page = 1\n",
    "    while(True):\n",
    "        # repos_url = \"https://api.github.com/repos/hakimel/reveal.js/issues?per_page=100&page=\"+str(page)        \n",
    "        repos_url = df_contri_rep['url'][ind]+\"/issues?q=per_page=100&page=\"+str(page)        \n",
    "        page += 1\n",
    "        gh_session = requests.Session()\n",
    "        gh_session.auth = (username, token)\n",
    "        \n",
    "        repos = json.loads(gh_session.get(repos_url).text)\n",
    "        # print(repos_url)\n",
    "        json_object = json.dumps(repos, indent = 4)\n",
    "        dict = json.loads(json_object)\n",
    "        df2 = json_normalize(dict)\n",
    "        # print(df2['pull_request.url'].tail())\n",
    "        # print(df2['pull_request.url'])\n",
    "        if(page == 1 and df2.shape[0]<50):\n",
    "            break\n",
    "        if(df2.shape[0] ==0):\n",
    "            break\n",
    "        if 'pull_request.url' in df2.columns:\n",
    "            df2[['url','repository_url','labels','created_at','id','number','pull_request.url']].to_csv('issues_all_coll.csv', mode='a', index=False, header=False)\n",
    "        else:\n",
    "            df2[['url','repository_url','labels','created_at','id','number']].to_csv('issues_all_coll.csv', mode='a', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_contri_rep = pd.read_csv('contri_rep.csv',usecols=['url'])\n",
    "all_issues_fin = pd.read_csv('all_issues_fin.csv',usecols=['repository_url','created_at'])\n",
    "all_issues_fin['created_at'] = pd.to_datetime(all_issues_fin['created_at'])\n",
    "print(df_contri_rep[\"url\"].nunique())\n",
    "print(all_issues_fin[\"repository_url\"].nunique())\n",
    "all_issues_count = all_issues_fin.groupby(all_issues_fin['created_at'].dt.year).agg(total = pd.NamedAgg('repository_url',aggfunc=pd.Series.nunique))\n",
    "\n",
    "all_issues_count['created_at']= all_issues_count.index\n",
    "all_issues_count['created_at'] = pd.to_datetime(all_issues_count['created_at'], format='%Y')\n",
    "print(all_issues_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd_r= pd.read_csv(\"issues_all_coll.csv\")\n",
    "# print(pd_r[pd_r['repository_url']=='https://api.github.com/repos/getredash/redash'])\n",
    "# print(pd_r)\n",
    "# print(pd_r[pd_r['pull_request.url'].notnull()])\n",
    "# print(pd_r[pd_r['pull_request.url'].isnull()])\n",
    "# print(pd_r[pd_r['pull_request.url'].isna()])\n",
    "# print(pd_r[pd_r['id']==1180734149])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_r[pd_r['pull_request.url'].isna()])\n",
    "print(pd_r[pd_r['pull_request.url'].notnull()])\n",
    "pd_fin = pd_r[pd_r['pull_request.url'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_fin.to_csv('all_issues_fin.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.io.json as pd_json\n",
    "all_issues_fin = pd.read_csv('all_issues_fin.csv')\n",
    "# pd.concat([all_issues_fin.drop(['labels'], axis=1), all_issues_fin['labels'].apply(pd.Series)], axis=1)\n",
    "all_issues_fin.columns\n",
    "# all_issues_fin['labels'].apply(pd.Series)\n",
    "# all_issues_fin.labels.json_normalize()\n",
    "# pd_json.json_normalize(data, record_path='data')\n",
    "# all_issues_fin.labels\n",
    "labels = all_issues_fin[all_issues_fin.labels.str.contains(\"good first issue\")]\n",
    "print(labels.shape)\n",
    "labels.to_csv('labels.csv', index=False, header=False)\n",
    "print(all_issues_fin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "all_issues_fin.labels\n",
    "# print(all_issues_fin[all_issues_fin.labels.str.contains(\"first\")][[\"url\",'repository_url','id', 'number','created_at']].shape)\n",
    "\n",
    "all_issues_fin[\"labels\"] = all_issues_fin[\"labels\"].apply(eval)\n",
    "# all_issues_fin.head(4).to_dict('labels')\n",
    "print(all_issues_fin.shape)\n",
    "# key = 'name'\n",
    "# value = 'good first issue candidate'\n",
    "# for l in all_issues_fin['labels']:    \n",
    "#     for d in l:\n",
    "#         print(d.keys)\n",
    "# out = [any(d.get(key) == value for d in l) for l in all_issues_fin['labels']]\n",
    "# (2951, 7)\n",
    "# slicing rows\n",
    "# all_issues_fin[out].shape\n",
    "\n",
    "# print(all_issues_fin['labels'].str['name'])\n",
    "# df = all_issues_fin.applymap(operator.itemgetter('labels'))\n",
    "# feature3 = [all_issues_fin.get('name') for d in all_issues_fin.labels]\n",
    "# print(feature3)\n",
    "# all_issues_fin[all_issues_fin.labels.str.contains(\"good first issue\")][[\"url\",'repository_url','id', 'number','created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfi_labels = ['good first issue candidate','good first issue', 'easy', 'Easy', 'low hanging fruit', 'minor bug', 'Easy Pick', 'Easy to Fix', 'good first bug',\n",
    "'beginner', 'good first contribution', 'Good first task', 'newbie', 'starter bug', 'beginner-task', 'Minor Bug','easy-pick',\n",
    "'minor feature', 'up-for-grabs', 'good-first-bug', 'easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues_fin[all_issues_fin.labels.str.contains(\"good first issue\")][[\"url\",'repository_url','id', 'number','created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3634, 7)\n",
      "(319769, 7)\n"
     ]
    }
   ],
   "source": [
    "key = 'name'\n",
    "out = [any(d.get(key) in gfi_labels for d in l) for l in all_issues_fin['labels']]\n",
    "df_gfis = all_issues_fin[out]\n",
    "print(df_gfis.shape)\n",
    "\n",
    "# reverse = ~pd.Series(out)\n",
    "# print(reverse)\n",
    "# df_others = all_issues_fin[reverse]\n",
    "rslt_df = all_issues_fin[~pd.Series(out)]\n",
    "# df_others = all_issues_fin[~df_gfis]\n",
    "print(rslt_df.shape)\n",
    "df_gfis.to_csv('gfi_coll.csv', index=False, header=False)\n",
    "df_gfis.to_csv('otheri_coll.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_unique = pd.DataFrame(df_gfis['repository_url'].unique(), columns=['repository_url'])\n",
    "url_unique = df_gfis['repository_url'].unique()\n",
    "print(url_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_unique = url_unique.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in url_unique.iterrows():\n",
    "#     row['repository_url']\n",
    "other_issues = rslt_df[rslt_df['repository_url'].isin(url_unique)]\n",
    "print(other_issues.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f52e72c7da3d38588779b7e0225782288049d1c57ba0ce139b57ffcecc10718"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
